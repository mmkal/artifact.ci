import pMap from 'p-map'
import {z} from 'zod'
import {client, sql} from '~/db'
import {createProxyClient} from '~/openapi/client'
import {paths} from '~/openapi/generated/supabase-storage'

const Env = z.object({
  SUPABASE_PROJECT_URL: z.string().url(),
  SUPABASE_SERVICE_ROLE_KEY: z.string().min(1),
})

export const createStorageClient = () => {
  const supabaseEnv = Env.parse(process.env)
  return createProxyClient<paths>().configure({
    baseUrl: `${supabaseEnv.SUPABASE_PROJECT_URL}/storage/v1`,
    headers: {
      apikey: supabaseEnv.SUPABASE_SERVICE_ROLE_KEY,
      authorization: `Bearer ${supabaseEnv.SUPABASE_SERVICE_ROLE_KEY}`,
    },
  })
}

export type FileInfo = {
  entry: {entryName: string; getData: () => Buffer}
  aliases: string[]
  mimeType: string
  jobPathname: string
}
export const insertFiles = async (dbArtifact: {id: string}, fileInfo: FileInfo[]) => {
  const storage = createStorageClient()
  const files = await pMap(
    fileInfo,
    async ({entry, aliases, mimeType, jobPathname}) => {
      console.log('uploading entry', entry.entryName, 'to', jobPathname, 'as', mimeType)
      const file = await storage.object
        .bucketName('artifact_files')
        .wildcard('github/job/' + jobPathname) // job pathname should be unique so use that in the storage backend. this is for convenience, we could use a random pathname here since we'll store the aliases in the db
        .post({content: {[mimeType]: entry.getData()}})

      console.log('uploaded', jobPathname, file.json.Id, file.json.Key)
      return {file, jobPathname, aliases}
    },
    {concurrency: 10},
  )

  const inserts = await client.any(sql<queries._void>`
    insert into file_aliases (alias, artifact_id, object_id)
    select alias, artifact_id, object_id
    from jsonb_populate_recordset(
      null::file_aliases,
      ${JSON.stringify(
        files.flatMap(f => {
          return f.aliases.flatMap(alias => ({
            alias,
            artifact_id: dbArtifact.id,
            object_id: f.file.json.Id,
          }))
        }),
      )}
    )
    on conflict (alias, object_id) do nothing
  `)

  return {files, inserts}
}

/** Searches for a file by its alias, and if found, fetches from supabase storage */
export const loadFile = async (filepath: string) => {
  const storage = createStorageClient()
  const dbFile = await client.maybeOne(sql<queries.DbFile>`
    select fa.object_id, o.name as storage_pathname
    from file_aliases fa
    join storage.objects o on fa.object_id = o.id::text
    where alias = ${filepath}
    and o.name is not null
    limit 1
  `) // ^ not sure why limit 1 is needed here?

  if (!dbFile || !dbFile.storage_pathname) {
    return null
  }

  const file = await storage.object.bucketName('artifact_files').wildcard(dbFile.storage_pathname).get()
  return {pathname: dbFile.storage_pathname, object: file}
}

export declare namespace queries {
  // Generated by @pgkit/typegen

  /** - query: `insert into file_aliases (alias, artifac... [truncated] ...n conflict (alias, object_id) do nothing` */
  export type _void = {}

  /** - query: `select fa.object_id, o.name as storage_p... [truncated] ...lias = $1 and o.name is not null limit 1` */
  export interface DbFile {
    /** column: `public.file_aliases.object_id`, not null: `true`, regtype: `text` */
    object_id: string

    /** column: `storage.objects.name`, regtype: `text` */
    storage_pathname: string | null
  }
}
