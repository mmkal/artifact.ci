import pMap from 'p-map'
import {z} from 'zod'
import {client, sql} from '~/db'
import {createProxyClient} from '~/openapi/client'
import {paths} from '~/openapi/generated/supabase-storage'
import {logger} from '~/tag-logger'

const Env = z.object({
  SUPABASE_PROJECT_URL: z.string().url(),
  SUPABASE_SERVICE_ROLE_KEY: z.string().min(1),
})

export const createStorageClient = () => {
  const supabaseEnv = Env.parse(process.env)
  return createProxyClient<paths>().configure({
    baseUrl: `${supabaseEnv.SUPABASE_PROJECT_URL}/storage/v1`,
    headers: {
      apikey: supabaseEnv.SUPABASE_SERVICE_ROLE_KEY,
      authorization: `Bearer ${supabaseEnv.SUPABASE_SERVICE_ROLE_KEY}`,
    },
  })
}

export type FileInfo = {
  entry: {entryName: string; getData: () => Buffer}
  aliases: string[]
  mimeType: string
}
export type ArtifactInfo = {
  id: string
  name: string
  repo: {owner: string; repo: string}
  created_at: Date
}
export const insertFiles = async (artifact: ArtifactInfo, fileInfo: FileInfo[]) => {
  const storage = createStorageClient()
  const artifactPathPrefix = [
    'artifacts',
    artifact.repo.owner,
    artifact.repo.repo,
    artifact.created_at.toISOString().split(/\D/).slice(0, 3).join('/'), // date part in subfolders so when debugging can navigate to year/month/day
    artifact.created_at.toISOString().split('T')[1].replaceAll(':', '.'), // time part as a dot-separated string
    artifact.name,
    artifact.id,
  ].join('/')

  fileInfo.forEach((f, i) => {
    const firstIndex = fileInfo.findIndex(o => f.entry.entryName === o.entry.entryName)
    if (firstIndex !== i) {
      logger.error('Duplicate entry', f.entry.entryName, firstIndex, i, fileInfo)
      throw new Error(`Duplicate entry: ${f.entry.entryName}`)
    }
  })
  const existing = await client.any(sql<queries.Object>`
    select name from storage.objects where ${artifact.id} = any(path_tokens)
  `)
  const existingNames = new Set(existing.map(o => o.name))
  const files = await pMap(
    fileInfo,
    async ({entry, aliases, mimeType}) => {
      return logger.run(`entry=${entry.entryName}`, async () => {
        const objectPath = `${artifactPathPrefix}/${entry.entryName}`
        if (existingNames.has(objectPath)) {
          logger.warn(`skipping duplicate file`)
          return {file: {json: {Id: objectPath, Key: objectPath}}, aliases}
        }

        const file = await storage.object
          .bucketName('artifact_files')
          .wildcard(objectPath)
          .post({content: {[mimeType]: entry.getData()}})
          .catch(e => {
            logger.error(`error uploading ${e}`)
            return {json: {Id: objectPath, Key: objectPath}}
          })

        return {file, aliases}
      })
    },
    {concurrency: 10},
  )

  const inserts = await client.any(sql<queries.FileAlias>`
    insert into file_aliases (alias, artifact_id, object_id)
    select alias, artifact_id, object_id
    from jsonb_populate_recordset(
      null::file_aliases,
      ${JSON.stringify(
        files.flatMap(f => {
          return f.aliases.flatMap(alias => ({
            alias,
            artifact_id: artifact.id,
            object_id: f.file.json.Id,
          }))
        }),
      )}
    )
    on conflict (alias, object_id) do nothing
    returning *
  `)

  return {files, inserts}
}

/** Searches for a file by its alias, and if found, fetches from supabase storage */
export const loadFile = async (filepath: string) => {
  const storage = createStorageClient()
  const dbFile = await client.maybeOne(sql<queries.DbFile>`
    select fa.object_id, o.name as storage_pathname
    from file_aliases fa
    join storage.objects o on fa.object_id = o.id::text
    where alias = ${filepath}
    and o.name is not null
    order by fa.created_at desc
    limit 1
  `)

  if (!dbFile || !dbFile.storage_pathname) {
    return null
  }

  const file = await storage.object.bucketName('artifact_files').wildcard(dbFile.storage_pathname).get()
  return {resolvedPathname: dbFile.storage_pathname, object: file}
}

export declare namespace queries {
  // Generated by @pgkit/typegen

  /** - query: `select name from storage.objects where $1 = any(path_tokens)` */
  export interface Object {
    /** column: `storage.objects.name`, regtype: `text` */
    name: string | null
  }

  /** - query: `insert into file_aliases (alias, artifac... [truncated] ...alias, object_id) do nothing returning *` */
  export interface FileAlias {
    /** column: `public.file_aliases.id`, not null: `true`, regtype: `prefixed_ksuid` */
    id: import('~/db').Id<'file_aliases'>

    /** column: `public.file_aliases.alias`, not null: `true`, regtype: `text` */
    alias: string

    /** column: `public.file_aliases.object_id`, not null: `true`, regtype: `text` */
    object_id: string

    /** column: `public.file_aliases.created_at`, not null: `true`, regtype: `timestamp with time zone` */
    created_at: Date

    /** column: `public.file_aliases.updated_at`, not null: `true`, regtype: `timestamp with time zone` */
    updated_at: Date

    /** column: `public.file_aliases.artifact_id`, not null: `true`, regtype: `prefixed_ksuid` */
    artifact_id: string
  }

  /** - query: `select fa.object_id, o.name as storage_p... [truncated] ...null order by fa.created_at desc limit 1` */
  export interface DbFile {
    /** column: `public.file_aliases.object_id`, not null: `true`, regtype: `text` */
    object_id: string

    /** column: `storage.objects.name`, regtype: `text` */
    storage_pathname: string | null
  }
}
